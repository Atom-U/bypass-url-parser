# Bypass Url Parser

Tool that tests `MANY` url bypasses to reach a `40X protected page`.

If you wonder why this code is `nothing but a dirty curl wrapper`, here's why:

- Most of the python requests do url/path/parameter encoding/decoding, and I hate this.
- If I submit raw chars, I want raw chars to be sent.
- If I send a weird path, I want it weird, not normalized.

This is `surprisingly hard` to achieve in python without loosing all of the lib goodies like parsing, ssl/tls encapsulation and so on. \
So, be like me, use `curl as a backend`, it's gonna be just fine.

Also, this tool can be used as a library, see [lib-sample-usage.py](lib-sample-usage.py)


## Usage

```
Bypass Url Parser, made with love by @TheLaluka
Improvements & Refactoring with the help of @jtop_fap
A tool that tests MANY url bypasses to reach a 40X protected page.

Usage:
    ./bypass-url-parser.py -u <URL> [(-m <mode>)...] [-o <outdir>] [-S <level>] [(-H <header>)...] [-r <num>]
                           [-s <ip>] [--spoofip-replace] [-p <port>] [--spoofport-replace] [--dump-payloads]
                           [-t <threads>] [-T <timeout>] [-x <proxy_url>] [-v | -d | -dd]

Program options:
    -H, --header <header>     Header(s) to use, format: "Cookie: can_i_haz=fire"
    -m, --mode <mode>         Bypass modes. See 'Bypasser.bypass_modes' in code [Default: all]
    -o, --outdir <outdir>     Output directory for results
    -x, --proxy <proxy_url>   Set a proxy in the format http://proxy_ip:port.
    -S, --save-level <level>  Save results level. From 0 (DISABLE) to 3 (FULL) [Default: 1]
    -s, --spoofip <ip>        IP(s) to inject in ip-specific headers
    -p, --spoofport <port>    Port(s) to inject in port-specific headers
    -r, --retry <num>         Retry attempts of failed requests. Set 0 to disable all retry tentatives [Default: 3]
    -t, --threads <threads>   Scan with N parallel threads [Default: 1]
    -T, --timeout <timeout>   Request times out after N seconds [Default: 5]
    -u, --url <URL>           URL (path is optional) to run bypasses against

General options:
    -h, --help                Show help, you are here :)
    -v, --verbose             Verbose output
    -d, --debug               Show more details like curl commands generated by this tool
    -dd, --debug              Print Debug level 2 (with all classes debug_class output)
    -V, --version             Show version info

Misc options:
    --spoofip-replace         Disable list of default internal IPs in 'http_headers_ip' bypass mode
    --spoofport-replace       Disable list of default internal ports in 'http_headers_port' bypass mode
    --dump-payloads           Dumps all payloads (curls) to /tmp/bup-payloads.lst.

Examples:
    ./bypass-url-parser.py -u "http://127.0.0.1/juicy_403_endpoint/" -s 8.8.8.8 -d
    ./bypass-url-parser.py -u /path/urls -t 30 -T 5 -H "Cookie: me_iz=admin" -H "User-agent: test"
```


## Expected result

```bash
./bypass-url-parser.py -u "http://thinkloveshare.com/api/jolokia/list" -t 20 -T 2 -S 2 -v
2022-07-27 14:18:07 work bup[12437] WARNING Stage: generate_curls for http://thinkloveshare.com/api/jolokia/list url
2022-07-27 14:18:14 work bup[12437] INFO Payloads to test: 2649
2022-07-27 14:18:14 work bup[12437] WARNING Stage: run_curls
Send curl requests: [████████████████████████████████████████████████████████████████████████████████████████████████████] 100.0% (20 threads, timeout 2s)
2022-07-27 14:19:11 work bup[12437] WARNING Triaged results & distinct pages for 'http://thinkloveshare.com/api/jolokia/list' url:
2022-07-27 14:19:11 work bup[12437] INFO
[#####] [bypass_method] [payload] => [status_code] [content_type] [content_length] [lines_count] [word_counts] [title] [server] [redirect_url] (filename)
[GROUP (2389)] [original_request] [] => [301] [text/html] [162] [7] [4] [301 Moved Permanently] [GitHub.com] [https://thinkloveshare.com/api/jolokia/list] (bypass-770e5322396930f5e6e6166c8d76ef48.html)
[GROUP (10)] [http_methods] [-X 'CONNECT'] => [405] [] [131] [5] [5] [405 Not Allowed] [Varnish] [] (bypass-4664f26a295212981877122fae274dc3.html)
[GROUP (13)] [http_headers_ip] [-H 'Content-Length: *'] => [413] [text/plain] [33] [1] [4] [] [Varnish] [] (bypass-3d9d56e24d0284ea2c78ebf031f3f755.html)
[GROUP (13)] [http_headers_ip] [-H 'Host: *'] => [404] [text/html] [9115] [11] [82] [Site not found &middot; GitHub Pages] [GitHub.com] [] (bypass-405eafe576175042d3615b2ad83eab38.html)
[GROUP (224)] [mid_paths] ['http://thinkloveshare.com/api/jolokia/..;%00/list'] => [400] [text/html] [9121] [11] [81] [Bad request &middot; GitHub Pages] [GitHub.com] [] (bypass-aff8205929cbf18f7356020c1f85dd21.html)
2022-07-27 14:19:11 work bup[12437] INFO Only relevant curl responses (results) were saved in the '/tmp/tmpxrdl861p-bypass-url-parser/' directory
2022-07-27 14:19:11 work bup[12437] INFO Also, inspect them manually with batcat:
echo /tmp/tmpxrdl861p-bypass-url-parser/{bypass-770e5322396930f5e6e6166c8d76ef48.html,bypass-4664f26a295212981877122fae274dc3.html,bypass-3d9d56e24d0284ea2c78ebf031f3f755.html, \
bypass-405eafe576175042d3615b2ad83eab38.html,bypass-aff8205929cbf18f7356020c1f85dd21.html} | xargs batcat
2022-07-27 14:19:11 work bup[12437] INFO Program log file which contains the results saved in /tmp/tmpxrdl861p-bypass-url-parser/triaged-bypass.log
```


## Setup

### LINUX

```bash
# Deps
sudo apt install -y bat curl virtualenv python3
# Tool
virtualenv -p python3 .py3
source .py3/bin/activate
pip install -r requirements.txt
python bypass-url-parser.py -u "http://thinkloveshare.com/juicy_403_endpoint/"
```

### DOCKER

```bash
docker build .
docker run --rm -it -v "$PWD:/host" "$BUILD_TAG" -u /host/urls.lst
```


## More about supported arguments

### Arguments parsing

Bypass-url-parser allows to define some arguments in many ways:

 - `-u, --url`, `-m, --mode`, `-s, --spoofip` and `-p, --spoofport` arguments can be a filename, a string, or a list (when `Bypasser` is used as a library);
 - `stdin` support for these three arguments (with `-`).
 
For example, if you want to define several target urls (`-u, --url`), all the following commands produce the same result:

```bash
./bypass-url-parser.py -u /path/urls
cat /path/urls | ./bypass-url-parser.py -u -
echo 'http://thinkloveshare.com/test' | ./bypass-url-parser.py -u -
```

### Bypass mode

If `-m, --mode` is specified, you can select the desired bypass mode to run a specific test (or tests) and reduce the number of requests sent by the tool.

For now, the following bypass mode(s) are supported:

```
all, mid_paths, case_substitution, char_encode, http_methods, http_headers_scheme, http_headers_ip, http_headers_port
```

Example: 

```bash
./bypass-url-parser.py -u /path/urls -m case_substitution -m char_encode -m http_headers_scheme
```

### Spoofip / Spoofport

In order to customize the ip addresses and ports used in bypass attempts, the tool supports the following options:

 - With `-s, --spoofip` you can set some IP(s) to inject into `ip-specific` headers (`X-Forwarded-For`, `X-Real-Ip`, etc.)
 - With `-p, --spoofport` you can set some ports to inject into `port-specific` headers (`X-Forwarded-Port`)
 
By default, these custom entries are added to the internal IP/port lists. If you want to use only your IP(s)/port(s), you can use `--spoofip-replace` and/or `--spoofport-replace` arguments.

Example: 

```bash
./bypass-url-parser.py -u /path/urls -s /path/custom_ip --spoofip-replace
./bypass-url-parser.py -u /path/urls -p 3000 -p 9443 -p 10443
```

### Results saving

By default, if target url is unique, the tool saves a copy of the results in `/tmp/tmpXXX-bypass-url-parser/triaged-bypass.log` log file. 

***Notes:** If multiple target urls are passed to `-u`, results are prefixed with the url as directory (`/tmp/tmpXXX-bypass-url-parser/http-target-com-8080-api-users/`).*

There are two arguments to customize this behavior:

 - `-o, --outdir` to set a custome output directory
 - `-S, --save-level` to choose a saving level
 
The saving levels are:


 - `0` (NONE): Disable output saving and output directory creation;
 - `1` (MINIMAL): Only save the program log file which contains the results (Default);
 - `2` (PERTINENT): Save the program log file and pertinent (results) curl responses in separate html files;
 - `3` (FULL): Save the program log file and all curl responses in separate html files.

Example: 

```bash
./bypass-url-parser.py -S 0
./bypass-url-parser.py -o /tmp/bypass-res 
./bypass-url-parser.py -o /tmp/bypass-res2 -S 2 -u "http://thinkloveshare.com/juicy_403_endpoint/"
tree /tmp/bypass-res2/
/tmp/bypass-res2/
├── bypass-3d9d56e24d0284ea2c78ebf031f3f755.html
├── bypass-405eafe576175042d3615b2ad83eab38.html
├── bypass-4664f26a295212981877122fae274dc3.html
├── bypass-770e5322396930f5e6e6166c8d76ef48.html
├── bypass-aff8205929cbf18f7356020c1f85dd21.html
└── triaged-bypass.log

0 directories, 6 files
```


## Non-Regression tests & Code Cleanup

```bash
# Code Cleanup
black .
# Ensure no regression is pushed
python bypass-url-parser.py -u "http://127.0.0.1:8000/foo/bar" -dd --dump-payloads
# Compare /tmp/bup-payloads.lst and the latest tests-history/bup-payloads-YYYY-MM-DD.lst
# TODO create ls/sort/diff bash command for maintainers
# Archive current test-set
mv /tmp/bup-payloads.lst tests-history/bup-payloads-YYYY-MM-DD.lst
# Commit & Merge if everything's clean & tested! :)
```